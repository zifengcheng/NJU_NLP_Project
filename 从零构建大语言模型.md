# 从零构建大语言模型
### 一、任务背景： 
本作业旨在让学生理解和实践大语言模型的构建和训练流程。我们将从基础开始，逐步构建一个简化版的大型语言模型，进行训练和调优。学生将从头开始实现模型架构，并探索模型的优化过程。特别是，探索在大语言模型中，残差连接（Residual Connections）和层归一化（Layer Normalization, LN）对模型性能的影响。

### 二、任务描述
#### (一) 基础任务（30分）
**任务目标：**

+ 从零开始实现一个小型的Transformer模型。
+ 完成模型的训练、损失函数的定义、优化算法的选择和训练过程的实现。
+ 在训练过程中，学生需要完成以下步骤：
    1. **模型架构实现**：使用PyTorch实现基本的Transformer层（自注意力机制、前馈网络、残差连接和层归一化）。
    2. **训练数据准备**：实现数据预处理过程，使用公开的文本数据集（如WikiText-2或自定义语料）进行训练。
    3. **训练过程**：编写训练脚本，跟踪训练损失，打印并绘制损失曲线（学生需在报告中展示训练过程中的损失曲线）。

**评分标准：**

+ 完整实现Transformer架构（15分）
+ 正确实现训练过程，损失曲线清晰（15分）

#### (二) 探索任务（30分）
**任务目标：**

+ 探索残差连接和层归一化（LN）在训练大语言模型时的作用。
+ 进行实验，比较不同配置（例如，有无残差连接、有无LN）的模型训练结果，分析其对模型性能的影响。
+ 学生需要提供实验结果，比较不同配置的模型在相同训练数据集上的性能（如训练损失、生成质量等）。

**实验设计：**

+ 设计不同的实验：例如，分别训练仅使用残差连接、仅使用层归一化、同时使用两者和没有这两者的模型。
+ 比较这些模型的训练损失、生成效果，并在报告中详细分析残差连接和LN的作用。

**评分标准：**

+ 清晰的实验设计和对比（15分）
+ 结果分析和结论（15分）

#### (三) 进阶任务（40分）
**任务目标：**

+ 针对训练的大语言模型，设计并实现一个优化方案，旨在提高模型的生成质量或减少训练时间。
+ 学生可以选择探索如LoRA（低秩适应）、蒸馏等技术来优化模型。
+ 提供一个完整的创新方案，并证明其相较于基础模型的优势。

**评分标准：**

+ 创新性和可行性的方案设计（20分）
+ 性能提升（20分）

### 三、提交要求：
+ **代码提交**：完整代码文件（包括模型的实现，训练脚本，实验结果等）。
+ **报告提交**：PDF格式的实验报告，包括：
    1. 模型架构设计与实现步骤
    2. 训练过程中的损失曲线
    3. 探索任务的实验结果与分析
    4. 进阶任务的优化方案设计与结果分析

### 四、注意事项：
+ 请勿直接下载现成的实现或直接使用预训练模型。作业要求从零开始实现。
+ 需要详细记录每次实验的设置、结果和分析，尤其是探索任务部分。
+ <font style="color:rgb(31, 35, 40);">联系方式：如有疑问，请联系 </font>qianly@smail.nju.edu.cn<font style="color:rgb(31, 35, 40);">；</font>

