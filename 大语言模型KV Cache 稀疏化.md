# 大作业题目：KV Cache 稀疏化策略在大语言模型推理中的应用研究

## 一、背景与动机

在大语言模型（LLMs）推理阶段，为了加速生成过程，模型会缓存历史 token 的 Key/Value 对（KV Cache）。但随着输入长度增长，KV Cache 的体积也迅速增加，带来显存和计算开销，成为推理效率的主要瓶颈。

本项目的目标是探索并评估不同粒度的 KV Cache 裁剪策略，在尽量保证生成质量的前提下，减少内存与延迟开销。

> 注意：本项目中，KV Cache 的裁剪不考虑 decode 阶段生成的 token，仅裁剪输入的序列。

## 二、基础任务要求（必做部分）

1. **实现至少两类 KV Cache 裁剪策略**

   - **Token 维度**
     - 保留最近和最远 N 个 token
     - 基于历史 attention 平均值筛选高权重 token 的 KV

   - **Head 维度**
     - 基于平均活跃度，为不同的 head 分配不同的保留数量

   - **Layer 维度**
     - 层间裁剪，基于平均活跃度，不同层保留不同数量

2. **在开源模型上部署并测试**

   - 推荐模型：Llama2-7B、Mistral-7B
   - 要求使用 PyTorch 框架写代码
   - 不重新计算被删除的 KV，仅支持一次性裁剪

## 三、进阶探索（选做部分）

- **自定义组合策略**
  - 如：先 token 剪枝再 head 剪枝等

- **可视化与分析**
  - attention map 对比图等

## 四、实验设置建议

| 维度         | 示例设置                              |
|--------------|---------------------------------------|
| 上下文长度   | 128 / 512 / 1024 / 2048 等（可自定） |
| 裁剪强度     | 自行设计                              |
| 数据集       | GSM8K、LongBench 等                   |
| 指标         | 裁剪强度 / 显存占用 / 正确率等        |

建议画出“精度-裁剪强度”的折线图，辅助分析策略有效性。

## 五、评分标准（总分 100 分）

| 模块             | 分数 | 内容说明                                                             |
|------------------|------|----------------------------------------------------------------------|
| 实现能力         | 25   | 能正确实现多种裁剪策略，并对推理产生实际影响                     |
| 实验设计与分析   | 25   | 设置合理，变量控制清晰，有图表对比与结论分析                     |
| 策略对比与讨论   | 20   | 对不同策略进行清晰比较，解释背后机制                             |
| 创新设计（进阶） | 20   | 有自定义裁剪思路或组合机制，展示深入理解                         |
| 报告与表达       | 10   | 表述清楚，结构清晰，有图表、文字与结论总结                       |

## 六、提交内容

### 代码

1. 代码结构不做要求  
2. 要给出能够运行的 python 环境（`requirements.txt`）  
3. 要给出能成功运行代码的脚本  

### 报告

1. 实现方法：详细描述实现方法  
2. 实验结果：展示不同条件下的生成结果  
3. 结果分析：对以上实验结果进行分析，你的方法起到了作用，如何起到的  

> 如有引用参考文献，请在报告中列出相关文献。  
> 如有疑问，请联系：liang_000821@163.com
