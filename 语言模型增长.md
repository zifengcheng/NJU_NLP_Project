# 语言模型增长 (Language Model Growth)

## 任务背景

Scaling Law 表明不断从零训练更大的模型可以获得更好的性能. 但是从零训练大模型需要消耗大量的计算资源. 模型增长 (Model Growth) 提供了一种通过小模型初始化大模型的解决方案. 通过这种初始化, 大模型可以更快地收敛, 从而减少训练大模型的资源消耗.

本课程大作业的目标是增长一个基于 BERT 的 token 分类模型, 并比较与直接微调大模型的训练效率.

## 任务描述

本课程大作业需要完成以下两部分任务.

第一部分为基础题:

1. 基于 WNUT 17 数据集微调 bert-small-uncased, 实现一个 token 分类模型.
2. 基于微调的 small 文本分类模型, 通过 Net2Net 方法增长出一个 bert-medium-uncased 和 bert-base-uncased 模型, 并继续训练.
3. 基于微调的 small 文本分类模型, 通过 bert2BERT 方法增长出一个 bert-medium-uncased 和 bert-base-uncased 模型, 并继续训练.

第二部分为进阶题:

1. 基于微调的 small 文本分类模型, 通过 LEMON 方法增长出一个 bert-medium-uncased 和 bert-base-uncased 模型, 并继续训练.
2. 探索更多的模型增长方法, 更快速的大模型训练.

评估对比模型时需要考虑对比训练过程中模型性能的变化.

## 提交要求

提交内容应当包括: 

- 完成代码的压缩包, 确保代码结构清晰可读;
- 实验报告 (pdf 格式).

## 注意事项

- 代码实现建议使用 huggingface transformer 库.
- 数据集可参考 https://huggingface.co/datasets/leondz/wnut_17 .
- 模型 bert-small-uncased 可参考 https://huggingface.co/gaunernst/bert-small-uncased .
- Net2Net 可参考论文 “Net2Net: Accelerating Learning via Knowledge Transfer”.
- bert2BERT 可参考论文 “bert2BERT: Towards Reusable Pretrained Language Models”.
- LEMON 请参考论文 “LEMON: Lossless model expansion”.
- 如有疑问, 请联系 [cw@smail.nju.edu.cn](mailto:cw@smail.nju.edu.cn) .
