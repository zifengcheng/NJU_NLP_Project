# 语言模型裁剪 (Language Model Pruning)

## 任务背景

预训练语言模型 (如: BERT) 通常因参数规模庞大而面临大量计算资源消耗和低推理效率的挑战. 模型裁剪 (Model Pruning) 是一种解决这一挑战的技术.

本课程大作业的目标是裁剪一个基于 BERT 的文本分类模型, 并分析比较模型的性能损失, 计算效率.

## 任务描述

本课程大作业需要完成以下两部分任务.

第一部分为基础题:

1. 基于 AG News 数据集微调 bert-base-uncased, 实现一个文本分类模型.
2. 基于微调的文本分类模型, 通过 Layer Pruning 技术裁剪模型.
3. 基于微调的文本分类模型, 通过 Head Pruning 技术裁剪模型.

第二部分为进阶题:

1. 基于微调的文本分类模型, 通过 CoFi 裁剪模型.
2. 探索更多的裁剪方法, 实现在不损失过多性能的情况下, 提升模型效率.

评估对比模型时需要考虑三个方面:
- 模型性能; 
- 模型参数量;
- 模型推理速度.

## 提交要求

提交内容应当包括:
- 完成代码的压缩包, 确保代码结构清晰可读;
- 实验报告 (pdf 格式).

## 注意事项

- 代码实现建议使用 huggingface transformer 库.
- 数据集参考 https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset .
- Layer Pruning 可参考论文 “Reducing Transformer depth on demand with structured dropout” 和 “Poor man’s BERT: Smaller and faster transformer models”.
- Head Pruning 可参考论文 “Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned” 和 “Are sixteen heads really better than one?”.
- CoFi 请参考论文 “Structured Pruning Learns Compact and Accurate Models”.
- 如有疑问, 请联系 [cw@smail.nju.edu.cn](mailto:cw@smail.nju.edu.cn) .
